{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelos para Detección de Fraude en Tarjetas de Crédito\n",
    "\n",
    "**Este notebook tiene como objetivo entrenar varios modelos de Machine Learning para la detección de fraude en tarjetas de crédito. A continuación, se describen los pasos realizados en el notebook.**\n",
    "\n",
    "## Importar Librerías y Funciones\n",
    "\n",
    "**Primero, importamos las librerías necesarias y las funciones definidas en el archivo `model_training.py` que se encuentra en la carpeta `src`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento.ipynb\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model_training import load_train_data, get_models, train_and_evaluate, save_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los Datos de Entrenamiento\n",
    "**Utilizamos la función `load_train_data` para cargar los datos de entrenamiento desde los archivos CSV procesados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento\n",
    "X_train, y_train = load_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir los Modelos\n",
    "**Utilizamos la función `get_models` para definir varios modelos de Machine Learning que se utilizarán para la detección de fraude.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar y Evaluar los Modelos\n",
    "**Utilizamos la función `train_and_evaluate` para entrenar los modelos definidos y evaluar su rendimiento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 14:04:20,305 - INFO - Entrenando Logistic Regression...\n",
      "2025-02-28 14:04:20,979 - INFO - Logistic Regression - F1-score: 0.9314\n",
      "2025-02-28 14:04:20,980 - INFO - Entrenando Random Forest...\n",
      "2025-02-28 14:07:07,052 - INFO - Random Forest - F1-score: 0.9886\n",
      "2025-02-28 14:07:07,052 - INFO - Entrenando XGBoost...\n",
      "2025-02-28 14:07:07,053 - INFO - Optimizando hiperparámetros de XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 14:10:09,543 - INFO - Mejores Hiperparámetros para XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "2025-02-28 14:10:09,544 - INFO - Mejor F1-score: 0.9984\n",
      "2025-02-28 14:10:14,997 - INFO - XGBoost - F1-score: 0.9991\n",
      "2025-02-28 14:10:14,999 - INFO - Entrenando LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 14:10:17,672 - INFO - LightGBM - F1-score: 0.9976\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y evaluar los modelos\n",
    "results, trained_models = train_and_evaluate(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar los Modelos Entrenados\n",
    "**Utilizamos la función `save_all_models` para guardar los modelos entrenados en la carpeta `models`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 14:10:24,197 - INFO - Mejor modelo basado en F1-score: XGBoost\n",
      "2025-02-28 14:10:24,199 - INFO - Modelo guardado: Logistic Regression en ../models/logistic_regression.pkl\n",
      "2025-02-28 14:10:24,230 - INFO - Modelo guardado: Random Forest en ../models/random_forest.pkl\n",
      "2025-02-28 14:10:24,232 - INFO - Modelo guardado: XGBoost en ../models/xgboost.pkl\n",
      "2025-02-28 14:10:24,237 - INFO - Modelo guardado: LightGBM en ../models/lightgbm.pkl\n",
      "2025-02-28 14:10:26,389 - INFO - Modelo XGBoost optimizado guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Guardar los modelos entrenados\n",
    "save_all_models(trained_models, results, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
